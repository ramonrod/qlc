{
 "metadata": {
  "name": "Create a translation graph"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown", 
     "source": [
      "# Translation Graphs", 
      "", 
      "The qlc repository has some scripts in the \"bin\" folder to extract and handle translation graphs from the quanthistling data. In addition to this, there is also one script that extracts a matrix directly from the data (rows are spanish stems, columns are translations in the sources). Such a a matrix may also be generated through translation graphs, by combining the following operations:", 
      "", 
      "1. Generating a translation graph from the data", 
      "2. Combining several translation graphs by matching spanish translations", 
      "3. linking spanish translations that have or contain the same stem", 
      "4. Parsing the graph, find all linked stems and print out a matrix with the connected nodes: spanish translation and their head words", 
      "", 
      "For each step there is one Python script to carry out the task. This allows intermediate processing steps with the translation graph either in Python or with other computational tools. Every Python script uses the \"dot\" file format to read and write graphs (see http://en.wikipedia.org/wiki/DOT_language). The scripts require Python 3."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "##What are translation graphs?", 
      "", 
      "In our case, translation graphs are graphs that connect all spanish translation with every head word that we find for each translation in our sources. The idea is that spanish is some kind of interlingua in our case: if a string of a spanish translation in one source matches a string in another source this will only be '''one''' node in our graph. For the head words, this is not the case: matching strings in head words in different source are different nodes in the graph. This holds even if the different sources describe the same language, as different sources will use different orthographies. ", 
      "", 
      "To fullfil that need, head words are internally represented as a string with two parts: the head word and its source. Both parts are seperated by a pipe symbol \"|\". For example, in a dot file such a node looks like this:", 
      "", 
      "> \"\u00f3c\u00e1ji|thiesen1998\" [lang=boa, source=thiesen1998_25_339];", 
      "", 
      "The square brackets contain additional attributes here. These attributes are not part of the node's name, they contain just additonal information the user wants to store with the nodes.", 
      "", 
      "In comparison, a spanish translation looks like this:", 
      "", 
      "> \"vaca\" [lang=spa];", 
      "", 
      "There is no attribute \"source\" here, as this translation might occur in several sources. An edge connecting the two nodes looks like this:", 
      "", 
      "> \"vaca\" -- \"\u00f3c\u00e1ji|thiesen1998\";", 
      "", 
      "To handle such graphs our scripts use the NetworkX Python library (http://networkx.lanl.gov/). It is kind of a standard in scientific graph computing with Python (remark: I started with the pygraph library, which has more or less the same API but by far not enough operations to compute with graphs later)."
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Imports"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "sys.path.append(\"../../qlc/src\")", 
      "from qlc.corpusreader import CorpusReaderDict",
      "from networkx import Graph", 
      "from qlc.translationgraph import read, write"
     ], 
     "language": "python", 
     "outputs": [
      {
       "ename": "NameError", 
       "evalue": "name 'sys' is not defined", 
       "output_type": "pyerr", 
       "traceback": [
        "<span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)", 
        "<span class=\"ansigreen\">h:\\ProjectsWin\\git-github\\qlc\\bin\\ipynb\\&lt;ipython-input-1-b684591fdd34&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>sys<span class=\"ansiyellow\">.</span>path<span class=\"ansiyellow\">.</span>append<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;../../qlc/src&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansigreen\">from</span> qlc<span class=\"ansiyellow\">.</span>CorpusReader <span class=\"ansigreen\">import</span> CorpusReaderDict<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansigreen\">from</span> networkx <span class=\"ansigreen\">import</span> Graph<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansigreen\">from</span> qlc<span class=\"ansiyellow\">.</span>translationgraph <span class=\"ansigreen\">import</span> read<span class=\"ansiyellow\">,</span> write<span class=\"ansiyellow\"></span>\n", 
        "<span class=\"ansired\">NameError</span>: name &apos;sys&apos; is not defined"
       ]
      }
     ], 
     "prompt_number": 1
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Step 1: Extracting a translation graph", 
      "", 
      "The fisrt step extract the data from Witotoan languages and returns a graph for each source."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "cr = CorpusReaderDict(\"../../qlc/data\")"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 4
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "import re", 
      "dictdata_ids = cr.dictdata_ids_for_component(\"Witotoan\")", 
      "re_quotes = re.compile('\"')"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 17
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "dictdata_ids"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 18, 
       "text": [
        "[u&apos;190&apos;, u&apos;186&apos;, u&apos;187&apos;, u&apos;185&apos;, u&apos;188&apos;, u&apos;189&apos;, u&apos;436&apos;]"
       ]
      }
     ], 
     "prompt_number": 18
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "graphs = list()", 
      "for dictdata_id in dictdata_ids:", 
      "    gr = Graph()", 
      "", 
      "    src_language_iso = cr.src_language_iso_for_dictdata_id(dictdata_id)", 
      "    tgt_language_iso = cr.tgt_language_iso_for_dictdata_id(dictdata_id)", 
      "    if src_language_iso != 'spa' and tgt_language_iso != 'spa':", 
      "        continue", 
      "    ", 
      "    language_iso = None", 
      "    if tgt_language_iso == 'spa':", 
      "        language_iso = src_language_iso", 
      "    else:", 
      "        language_iso = tgt_language_iso", 
      "", 
      "    dictdata_string = cr.dictdata_string_id_for_dictata_id(dictdata_id)", 
      "    bibtex_key = dictdata_string.split(\"_\")[0]", 
      "", 
      "    for head, translation in cr.heads_with_translations_for_dictdata_id(dictdata_id):", 
      "        if src_language_iso == 'spa':", 
      "            (head, translation) = (translation, head)", 
      "", 
      "        head_with_source = re_quotes.sub('', u\"{0}|{1}\".format(head, bibtex_key))", 
      "        translation = re_quotes.sub('', translation)", 
      "        gr.add_node(head_with_source, attr_dict={ \"lang\": language_iso, \"source\": bibtex_key })", 
      "        gr.add_node(translation, attr_dict={ \"lang\": \"spa\" })", 
      "        gr.add_edge(head_with_source, translation)", 
      "    graphs.append(gr)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 19
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "###Test"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "import networkx", 
      "networkx.algorithms.components.number_connected_components(graphs[0])"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 20, 
       "text": [
        "2257"
       ]
      }
     ], 
     "prompt_number": 20
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Step 2: Combining graphs", 
      "", 
      "Combine graphs by matching spanish translations."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "import copy", 
      "combined_graph = copy.deepcopy(graphs[0])", 
      "for gr in graphs[1:]:", 
      "    for node in gr:", 
      "        combined_graph.add_node(node, gr.node[node])", 
      "    for n1, n2 in gr.edges_iter():", 
      "        combined_graph.add_edge(n1, n2, gr.edge[n1][n2])"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 21
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "###Test"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "networkx.algorithms.components.number_connected_components(combined_graph)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 22, 
       "text": [
        "15279"
       ]
      }
     ], 
     "prompt_number": 22
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Step 3: Connect nodes with the same stem", 
      "", 
      "The next step is to connect spanish translations that contain the same stem. For this, the script \"translations_spanish_graph_connectstemswithoutstopwords.py\" first removes certain stop words from the translation (list of stop words is here: data/stopwords/spa.txt). There are two cases then: just one word remains, or more than one word remains.", 
      "", 
      "The script has an option to let the user choose what to do with the latter: either they are not connected with anything at all (default behaviour), or each word is stemmed and the translation is connected with every other translation that contain the same stems. Right now this results in many connections that look not very useful. This should be done in a more intelligent way in the future (for example find heads of phrases in mulitword expression and only connect those; split the weight of the connections between all stems and work with weighted graphs from this step on; ...).", 
      "", 
      "To connect the spanish translations the script adds additional \"stem nodes\" to the graph. The name of these nodes consists of a spanish word stem plus a pipe symbol plus the string \"stem\". These nodes look like this in a dot file:", 
      "", 
      "> \"tom|stem\" [is_stem=True];", 
      "", 
      "The introduction of these nodes later facilites the output of translation matrixes, as you can just search for stems within the graph and only output direct neighbours with spanish translations. It would also be possible to directly connect the spanish translations if they have a matching stem, but then the graph traversal to find matching translations and their heads is a bit more complicated later. If someone has some ideas about this..."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "import codecs, unicodedata, qlc.utils", 
      "", 
      "stopwords = qlc.utils.stopwords_from_file(\"../../src/qlc/data/stopwords/spa.txt\")"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 23
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "combined_graph_stemmed = copy.deepcopy(combined_graph)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 24
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "from nltk.stem.snowball import SpanishStemmer", 
      "stemmer = SpanishStemmer()", 
      "split_multiwords = False"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 25
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "for node in combined_graph.nodes():", 
      "    if \"lang\" in combined_graph.node[node] and combined_graph.node[node][\"lang\"] == \"spa\":", 
      "        phrase_without_stopwords = qlc.utils.remove_stopwords(node, stopwords)", 
      "        phrase_stems = qlc.utils.stem_phrase(phrase_without_stopwords, stemmer, split_multiwords)", 
      "        for stem in phrase_stems:", 
      "            stem = stem + \"|stem\"", 
      "            combined_graph_stemmed.add_node(stem, is_stem=True)", 
      "            combined_graph_stemmed.add_edge(stem, node)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 26
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Test"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "networkx.algorithms.components.number_connected_components(combined_graph_stemmed)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 27, 
       "text": [
        "11973"
       ]
      }
     ], 
     "prompt_number": 27
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Output .dot file"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "OUT = codecs.open(\"IPython/frontend/html/notebook/static/translation_graph_stemmed.dot\", \"w\", \"utf-8\")", 
      "OUT.write(write(combined_graph_stemmed))", 
      "OUT.close()"
     ], 
     "language": "python", 
     "outputs": [
      {
       "ename": "UnicodeEncodeError", 
       "evalue": "'ascii' codec can't encode character u'\\u0301' in position 8: ordinal not in range(128)", 
       "output_type": "pyerr", 
       "traceback": [
        "<span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">UnicodeEncodeError</span>                        Traceback (most recent call last)", 
        "<span class=\"ansigreen\">/home/qlc/ipython/&lt;ipython-input-28-a52bc5d97b8a&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> OUT <span class=\"ansiyellow\">=</span> codecs<span class=\"ansiyellow\">.</span>open<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;IPython/frontend/html/notebook/static/translation_graph_stemmed.dot&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;w&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;utf-8&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>OUT<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">(</span>write<span class=\"ansiyellow\">(</span>combined_graph_stemmed<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> OUT<span class=\"ansiyellow\">.</span>close<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n", 
        "<span class=\"ansigreen\">/home/qlc/qlc/src/qlc/translationgraph.pyc</span> in <span class=\"ansicyan\">write</span><span class=\"ansiblue\">(gr)</span>\n<span class=\"ansigreen\">     86</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     87</span>     <span class=\"ansigreen\">for</span> n <span class=\"ansigreen\">in</span> gr<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 88</span><span class=\"ansiyellow\">         </span>node_string <span class=\"ansiyellow\">=</span> <span class=\"ansiblue\">&apos;&quot;{0}&quot;&apos;</span><span class=\"ansiyellow\">.</span>format<span class=\"ansiyellow\">(</span>n<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     89</span>         node_attributes <span class=\"ansiyellow\">=</span> gr<span class=\"ansiyellow\">.</span>node<span class=\"ansiyellow\">[</span>n<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     90</span>         <span class=\"ansigreen\">if</span> len<span class=\"ansiyellow\">(</span>node_attributes<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">&gt;</span> <span class=\"ansicyan\">0</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n", 
        "<span class=\"ansired\">UnicodeEncodeError</span>: &apos;ascii&apos; codec can&apos;t encode character u&apos;\\u0301&apos; in position 8: ordinal not in range(128)"
       ]
      }
     ], 
     "prompt_number": 28
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Step 4: Output a matrix", 
      "", 
      "The last step then outputs a matrix for a given translation graph. ", 
      "", 
      "The code outputs the matrix to a file \"translation_matrix.csv\" in this case. For this, the code first finds all nodes that are stems and collect all the neighbours that are spanish translations. This is one row in the matrix. It then collects all neighbours of each spanish translations that are neither stems nor spanish translations (i.e. the head words of the sources). Each source's head words is then one column entry of that row. Several translation or head words are seperated by a pipe symbol \"|\".", 
      ""
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "import collections", 
      "matrix = {}", 
      "sources = set()", 
      "for node in combined_graph_stemmed:", 
      "    if \"is_stem\" in combined_graph_stemmed.node[node] and combined_graph_stemmed.node[node][\"is_stem\"]:", 
      "        spanish_nodes = [n for n in combined_graph_stemmed[node] if \"lang\" in combined_graph_stemmed.node[n] and combined_graph_stemmed.node[n][\"lang\"] == \"spa\"]", 
      "        head_nodes = []", 
      "        for sp in spanish_nodes:", 
      "            head_nodes += [n for n in combined_graph_stemmed[sp] if (\"lang\" not in combined_graph_stemmed.node[n] or combined_graph_stemmed.node[n][\"lang\"] != \"spa\") and (\"is_stem\" not in combined_graph_stemmed.node[n] or not combined_graph_stemmed.node[n][\"is_stem\"])]", 
      "        head_nodes = set(head_nodes)", 
      "", 
      "        heads = collections.defaultdict(list)", 
      "        for head in head_nodes:", 
      "            (head, source) = head.split(\"|\")", 
      "            sources.add(source)", 
      "            heads[source].append(head)", 
      "        matrix[\"|\".join(sorted(spanish_nodes))] = heads"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": "&nbsp;"
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "OUT = codecs.open(\"IPython/frontend/html/notebook/static/translation_matrix.csv\", \"w\", \"utf-8\")", 
      "sorted_sources = sorted(sources)", 
      "OUT.write(\"{0}\\t{1}\\n\".format(\"spa\", \"\\t\".join(sorted_sources)))", 
      "for spanish in sorted(matrix):", 
      "    OUT.write(spanish)", 
      "    OUT.write(\"\\t\")", 
      "    sources_heads = []", 
      "    for source in sorted(sources):", 
      "        heads = [h for h in matrix[spanish][source]]", 
      "        sources_heads.append(\"|\".join(sorted(heads)))", 
      "    OUT.write(\"\\t\".join(sources_heads))", 
      "    OUT.write(\"\\n\")", 
      "OUT.close()"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": "&nbsp;"
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Download Matrix File", 
      "", 
      "<a href=\"static/translation_matrix.csv\" target=\"_blank\">Link to matrix file as CSV</a>"
     ]
    }
   ]
  }
 ]
}